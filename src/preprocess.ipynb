{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9aa372e",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline\n",
    "**Tokenization**: in my experiment i'm going to consider tokens as words in a tweet separated by whitespace.\n",
    "\n",
    "Punctuation is dropped, except for the following:\n",
    "- Keep \"?\" and \"!\" because they are import and often find in surprise or anger tweets. \n",
    "- Keep \"@\" because Users are different from i.e words that may be contained in the users. \n",
    "- Keep \"#\" because hashtag are different and carry a different meaning from word. \n",
    "- I keep \".\" for elipses \"...\" that also carry emotional information.\n",
    "\n",
    "The following class incapsulate the logic of the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67499b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c762801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessPipeline:\n",
    "  def __init__(self):\n",
    "    self.tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)\n",
    "\n",
    "    # List of chars to keep\n",
    "    # point is keep for elipses \"...\"\n",
    "    chars_to_keep = \"@#?!.'_\"\n",
    "    self.punct_to_remove = \"\".join([c for c in string.punctuation if c not in chars_to_keep])\n",
    "\n",
    "  def clean_text(self, text):\n",
    "    # Converts ðŸ˜‚ to \" :face_with_tears_of_joy: \"\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\b(href|http|https)\\b', '', text)\n",
    "\n",
    "    # Some noise patterns found\n",
    "    noise_patterns = [\n",
    "        r'gt',\n",
    "        r'class[^\\w\\s]*delicious[^\\w\\s]*title[^\\w\\s]*share[^\\w\\s]*del', # Removes 'gt' (from >)\n",
    "        r'rel[^\\w\\s]*nofollow[^\\w\\s]*target[^\\w\\s]*blank',              # Specific CSS/HTML string\n",
    "        r'languagedirection[^\\w\\s]*ltr',                                 # Specific CSS/HTML string\n",
    "        r'\\b(type|application|atom|xml|feedlinks|href|http|https)\\b',     # Directional metadata\n",
    "    ]\n",
    "\n",
    "    combined_noise = '|'.join(noise_patterns)\n",
    "    text = re.sub(combined_noise, '', text)\n",
    "\n",
    "    # Remove puntuation, keep some special characters\n",
    "    # We use a translation table here; it's much faster than regex for single characters\n",
    "    table = str.maketrans('', '', self.punct_to_remove)\n",
    "    text = text.translate(table)\n",
    "\n",
    "    text = re.sub(combined_noise, '', text) # re apply\n",
    "\n",
    "    # Remove extra space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "  def transform(self, text):\n",
    "    text = self.clean_text(text)\n",
    "    tokens = self.tweet_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
