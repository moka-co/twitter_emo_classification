{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a290f181",
      "metadata": {
        "id": "a290f181"
      },
      "source": [
        "Note: i'm using colab for training, so i need to import my github repository and do all the steps here\n",
        "\n",
        "# Clone Repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/MyLabs/"
      ],
      "metadata": {
        "id": "OSdA3hA70mlL",
        "outputId": "f78766dd-33bd-45bd-b5c5-0c49a27c406d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OSdA3hA70mlL",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/moka-co/twitter_emo_classification.git\n",
        "!git pull\n",
        "\n",
        "%cd twitter_emo_classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf41dId7s-ga",
        "outputId": "14965465-cf37-4728-dd5e-cf7317e5179d"
      },
      "id": "Rf41dId7s-ga",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'twitter_emo_classification'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 173 (delta 82), reused 94 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (173/173), 23.43 MiB | 15.44 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "/content/drive/MyDrive/MyLabs/twitter_emo_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .\n",
        "\n",
        "!chmod +x scripts/run_scripts.sh\n",
        "#!./scripts/run_scripts.sh"
      ],
      "metadata": {
        "id": "iyECOjL7tNaU"
      },
      "id": "iyECOjL7tNaU",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m scripts.download_data\n"
      ],
      "metadata": {
        "id": "niFEOVaZ40pr",
        "outputId": "611458cb-6f9e-4a63-8c73-23879a3cf942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "niFEOVaZ40pr",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total_size= total_size\n",
            "data/glove/glove.6B.zip: 100% 411G/411G [04:11<00:00, 3.43MiG/s]\n",
            "Downloaded data/glove/glove.6B.zip succesfully\n",
            "Extraction complete! Files are in: data/glove/\n",
            "Successfully download and extracted Glove model under data/glove/ directory\n",
            "Successfully downloaded Semeval Dataset under data/datasets/raw\n",
            "Successfully downloaded ELTEA17 Dataset under data/datasets/raw\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/aadyasingh55/twitter-emotion-classification-dataset?dataset_version_number=1...\n",
            "100% 22.3M/22.3M [00:02<00:00, 9.31MB/s]\n",
            "Extracting files...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull\n",
        "!python -m scripts.merge_data\n",
        "!ls -R data/datasets/process\n",
        "#!python -m scripts.remove_outliers"
      ],
      "metadata": {
        "id": "WlpOzX3S0U_Q",
        "outputId": "e0d2cd30-7a0d-436f-c498-09c12fc171ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WlpOzX3S0U_Q",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 529 bytes | 9.00 KiB/s, done.\n",
            "From https://github.com/moka-co/twitter_emo_classification\n",
            "   5ea2ae5..a21edcf  main       -> origin/main\n",
            "Updating 8f43c9f..a21edcf\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tscripts/run_scripts.sh\n",
            "Please commit your changes or stash them before you merge.\n",
            "Aborting\n",
            "Filtered 591 rows out of 6838\n",
            "Database successfully saved\n",
            "ls: cannot access 'data/datasets/process': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0QfZujzB6oiD"
      },
      "id": "0QfZujzB6oiD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set seed for reproducibility"
      ],
      "metadata": {
        "id": "26zHlmGTvFPj"
      },
      "id": "26zHlmGTvFPj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "301e763b",
      "metadata": {
        "id": "301e763b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "a72HCTHTvG4x"
      },
      "id": "a72HCTHTvG4x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a7a193c7",
      "metadata": {
        "id": "a7a193c7"
      },
      "source": [
        "## Vocabulary Construction\n",
        "\n",
        "- Words are initialized with GloVe weight if the word is present\n",
        "- otherwise they are initialized with zero.\n",
        "\n",
        "Considering just a small subset i.e only the words present in the dataset, instead of full GloVe greatly improved the memory footpring from 480 MB (full GloVe) to less for 80k tokens.\n",
        "\n",
        "For Out of Vocabulary words, they are initialized with zero and their embedding are learned during the training.\n",
        "\n",
        "Since the dataset comes from twitter, i expect to have some out of vocabulary words because of twitter slangs.\n",
        "\n",
        "Example: \"sick\" in GloVe (illness) vs tweets (slang for \"awesome\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that defines a glove embeddings matrix\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    path: path to glove.6B.100d.txt\n",
        "    word2idx: dictionary mapping words to integers from your dataset\n",
        "    \"\"\"\n",
        "    vocab_size = len(word2idx)\n",
        "    # Initialize matrix with random values (or zeros)\n",
        "    embedding_matrix = torch.randn(vocab_size, embedding_dim)\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if word in word2idx:\n",
        "                vector = torch.tensor([float(x) for x in values[1:]])\n",
        "                idx = word2idx[word]\n",
        "                embedding_matrix[idx] = vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "4BXCy5S_vPdV"
      },
      "id": "4BXCy5S_vPdV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7997ede",
      "metadata": {
        "id": "b7997ede",
        "outputId": "db78e493-e44d-440d-bad1-9b61dd610ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocabulary with 79287 tokens.\n"
          ]
        }
      ],
      "source": [
        "# Load vocabulary\n",
        "vocabulary_path = \"../data/vocab.json\"\n",
        "\n",
        "with open(vocabulary_path, 'r', encoding='utf-8') as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "print(f\"Loaded vocabulary with {len(word2idx)} tokens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506ddacd",
      "metadata": {
        "id": "506ddacd"
      },
      "outputs": [],
      "source": [
        "class EmoDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = [torch.tensor(s) for s in sequences]\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a9237e",
      "metadata": {
        "id": "62a9237e",
        "outputId": "75cb08a6-42d5-4783-bf11-2b4f379e4aea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>emotions</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>token_count</th>\n",
              "      <th>sequences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i feel awful about it too because it s my job ...</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "      <td>[i, feel, awful, about, it, too, because, it, ...</td>\n",
              "      <td>26</td>\n",
              "      <td>[2, 3, 455, 28, 13, 94, 38, 13, 85, 11, 330, 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im alone i feel awful</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "      <td>[im, alone, i, feel, awful]</td>\n",
              "      <td>5</td>\n",
              "      <td>[17, 218, 2, 3, 455]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ive probably mentioned this before but i reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>joy</td>\n",
              "      <td>[ive, probably, mentioned, this, before, but, ...</td>\n",
              "      <td>27</td>\n",
              "      <td>[74, 317, 1377, 23, 168, 21, 2, 40, 39, 3, 387...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i was feeling a little low few days back</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "      <td>[i, was, feeling, a, little, low, few, days, b...</td>\n",
              "      <td>9</td>\n",
              "      <td>[2, 22, 8, 7, 56, 405, 190, 165, 101]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i beleive that i am much more sensitive to oth...</td>\n",
              "      <td>2</td>\n",
              "      <td>love</td>\n",
              "      <td>[i, beleive, that, i, am, much, more, sensitiv...</td>\n",
              "      <td>18</td>\n",
              "      <td>[2, 16852, 10, 2, 24, 77, 37, 1808, 5, 120, 15...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label emotions  \\\n",
              "0  i feel awful about it too because it s my job ...      0  sadness   \n",
              "1                              im alone i feel awful      0  sadness   \n",
              "2  ive probably mentioned this before but i reall...      1      joy   \n",
              "3           i was feeling a little low few days back      0  sadness   \n",
              "4  i beleive that i am much more sensitive to oth...      2     love   \n",
              "\n",
              "                                      processed_text  token_count  \\\n",
              "0  [i, feel, awful, about, it, too, because, it, ...           26   \n",
              "1                        [im, alone, i, feel, awful]            5   \n",
              "2  [ive, probably, mentioned, this, before, but, ...           27   \n",
              "3  [i, was, feeling, a, little, low, few, days, b...            9   \n",
              "4  [i, beleive, that, i, am, much, more, sensitiv...           18   \n",
              "\n",
              "                                           sequences  \n",
              "0  [2, 3, 455, 28, 13, 94, 38, 13, 85, 11, 330, 5...  \n",
              "1                               [17, 218, 2, 3, 455]  \n",
              "2  [74, 317, 1377, 23, 168, 21, 2, 40, 39, 3, 387...  \n",
              "3              [2, 22, 8, 7, 56, 405, 190, 165, 101]  \n",
              "4  [2, 16852, 10, 2, 24, 77, 37, 1808, 5, 120, 15...  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_path = \"../data/datasets/final/dataset.parquet\"\n",
        "df = pd.read_parquet(dataset_path)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972729ef",
      "metadata": {
        "id": "972729ef"
      },
      "outputs": [],
      "source": [
        "# Divide dataset into train and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# x = your sequences (list of lists of integers)\n",
        "# y = your labels (0 to 5)\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
        "    df['sequences'].values,\n",
        "    df['label'].values,\n",
        "    test_size=0.2,          # 20% for testing\n",
        "    random_state=42,        # For reproducibility\n",
        "    stratify=df['label'].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1c8071",
      "metadata": {
        "id": "fe1c8071"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Sort by length (optional but helps LSTM efficiency)\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    # Pad sequences to the length of the longest one in this batch\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return padded_sequences, labels\n",
        "\n",
        "# Define seed for reproducibility\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# Create the final DataLoaders\n",
        "BATCH_SIZE=32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    EmoDataset(train_sequences, train_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    EmoDataset(test_sequences, test_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1c374f",
      "metadata": {
        "id": "6e1c374f"
      },
      "outputs": [],
      "source": [
        "# Define train, validate and compute top-k function\n",
        "def train(model, train_loader, device, optimizer, criterion):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "        # Cast labels to long (int64) which is required by CrossEntropyLoss\n",
        "        inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        if hasattr(model, 'attention'):\n",
        "          outputs, _ = model(inputs) # this is different for baseline and lstm model\n",
        "        else:\n",
        "          outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  train_acc = 100 * correct_train / total_train\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "def top_k_accuracy(output, target, k=2):\n",
        "  \"\"\"\n",
        "  Computes the accuracy over the k top predictions\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "      batch_size = target.size(0)\n",
        "\n",
        "      # Get the indices of the top k predictions\n",
        "      # _, pred shape: [batch_size, k]\n",
        "      _, pred = output.topk(k, 1, True, True)\n",
        "\n",
        "      # Transpose to [k, batch_size] to compare with target\n",
        "      pred = pred.t()\n",
        "\n",
        "      # Compare pred with target (target is broadcasted)\n",
        "      # correct shape: [k, batch_size] (Boolean)\n",
        "      correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "      # Sum the correct predictions\n",
        "      correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "      return correct_k.item()\n",
        "\n",
        "\n",
        "def validate(model, test_loader, device, criterion):\n",
        "  model.eval()\n",
        "\n",
        "  val_loss = 0.0 # Validation Loss\n",
        "  correct_val = 0 # Accuracy counter\n",
        "  correct_topk = 0 # Top-k Accuracy counter\n",
        "  total_val = 0 # Total validation loss\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "          # Cast labels to long for validation as well\n",
        "          inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "\n",
        "          # Get output\n",
        "          if hasattr(model, 'attention'):\n",
        "            outputs, _ = model(inputs) # this is different for baseline and lstm model\n",
        "          else:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "          # Standard Validation Loss\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "          # Standard Accuracy (Top-1)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_val += labels.size(0)\n",
        "          correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "          # Top-2 accuracy\n",
        "          correct_topk += top_k_accuracy(outputs, labels, k=2)\n",
        "\n",
        "          all_preds.extend(predicted.cpu().numpy())\n",
        "          all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  val_loss = val_loss / len(test_loader)\n",
        "  val_acc = 100 * correct_val / total_val\n",
        "  topk_acc = 100 * correct_topk / total_val\n",
        "  f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "  f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "  return val_loss, val_acc, f1_weighted, f1_macro, topk_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e83682",
      "metadata": {
        "id": "d1e83682"
      },
      "source": [
        "## Weighted CrossEntropy Loss\n",
        "Cross Entropy Loss is a loss function used for classification, defined as:\n",
        "$$\n",
        "\\ell(\\hat{\\mathbf y}, t) = -\\log \\hat{y}_t = -\\sum_i p(i)\\log \\hat{y}_i\n",
        "$$\n",
        "- $p(i)=1[i-t]$ is one hot vector representing true labels distributions\n",
        "\n",
        "Since the problem is unbalanced, i weighted the cross entropy loss in such way:\n",
        "1. $w_i$ weights are chosen inversely proportional to class frequency\n",
        "2. Cross Entropy Loss is initialized with this bias: `nn.CrossEntropyLoss(weight=class_weights.to(device))`\n",
        "\n",
        "In this way the loss function becomes:\n",
        "$$\n",
        "\\ell(\\hat{\\mathbf y}, t) = - w_t \\log \\hat{y}_t = -\\sum_i w_i \\; p(i)\\log \\hat{y}_i\n",
        "$$\n",
        "\n",
        "This penalizes the model more if it missclassifies rarer classes rather than common ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab32bf6",
      "metadata": {
        "id": "3ab32bf6"
      },
      "outputs": [],
      "source": [
        "# Load TOML configuration for Hyperparameters\n",
        "import tomllib\n",
        "load_path_toml = \"../config.toml\"\n",
        "def load_config(path=load_path_toml):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return tomllib.load(f)\n",
        "\n",
        "config = load_config()\n",
        "\n",
        "# Save shared configuration parameters\n",
        "embedding_dim = config[\"project\"][\"embedding_dim\"]\n",
        "output_dim = config[\"project\"][\"output_dim\"]\n",
        "epoch_num = config[\"project\"][\"epoch_num\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1afb7b5",
      "metadata": {
        "id": "b1afb7b5"
      },
      "source": [
        "# Train Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e6dd2b",
      "metadata": {
        "id": "a0e6dd2b"
      },
      "outputs": [],
      "source": [
        "lr = config[\"model\"][\"baseline\"][\"lr\"]\n",
        "wd = config[\"model\"][\"baseline\"][\"wd\"]\n",
        "hidden_dim = config[\"model\"][\"baseline\"][\"hidden_dim\"]\n",
        "\n",
        "# 1. Hyperparameters\n",
        "hyps = {\n",
        "    \"embedding_dim\" : embedding_dim, # Embedding dimension\n",
        "    \"hidden_dim\" : 256, # Hidden dimension\n",
        "    \"output_dim\" : output_dim, # Output dimension\n",
        "    \"lr\" : lr, # Learning rate\n",
        "    \"wd\" : wd, # weight decay\n",
        "    \"epoch_num\" : epoch_num,\n",
        "    } # Number of training epoch\n",
        "\n",
        "# 2. Create obj glove weight matrix and model\n",
        "weights = load_glove_embeddings('glove.6B.300d.txt', word2idx, embedding_dim=hyps[\"embedding_dim\"])\n",
        "model_base = EmotionClassifierBaseline(len(word2idx), # Dict\n",
        "                                       hyps[\"embedding_dim\"],\n",
        "                                       hyps[\"hidden_dim\"],\n",
        "                                       hyps[\"output_dim\"],\n",
        "                                       weights, # Glove Weights\n",
        "                                       distributions=list(distributions) # Prior Initialization\n",
        "                                       )\n",
        "\n",
        "# 3. Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_base = model_base.to(device)\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "# add bias to Cross Entropy Loss\n",
        "freqs = torch.tensor(list(distributions))\n",
        "class_weights = 1.0 / freqs\n",
        "class_weights = class_weights / class_weights.sum() * len(freqs)\n",
        "criterion_base = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "optimizer_base = torch.optim.Adam(model_base.parameters(), lr=hyps['lr'], weight_decay=hyps[\"wd\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8c6d53",
      "metadata": {
        "id": "5b8c6d53"
      },
      "outputs": [],
      "source": [
        "# Save F1-Score history, both weighted and macro\n",
        "history_base = {'weighted': [], 'macro': [], 'train_loss': [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience_limit = 5\n",
        "\n",
        "# Train loop\n",
        "try:\n",
        "  for epoch in range(hyps[\"epoch_num\"]):\n",
        "    # Calls train and validate custom functions\n",
        "    train_loss, train_acc = train(model_base, train_loader, device, optimizer_base, criterion_base)\n",
        "    val_loss, val_acc, f1_weighted, f1_macro, topk_acc = validate(model_base, test_loader, device, criterion_base)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Epoch [{epoch+1}/{hyps[\"epoch_num\"]}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%,  '\n",
        "          f\"Top-2 Val Accuracy: {topk_acc:.2f}%, \"\n",
        "          f'Weighted F1-Score: {f1_weighted:.4f},  '\n",
        "          f'Macro F1-Score: {f1_macro:.4f},  ')\n",
        "\n",
        "    # Add results to history dictionary\n",
        "    history_base['weighted'].append(f1_weighted)\n",
        "    history_base['macro'].append(f1_macro)\n",
        "    history_base['train_loss'].append(train_loss)\n",
        "    history_base['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    patience_counter += 1\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        file_path_base = f\"./best_model_baseline_v1_f1_{f1_macro:.4f}.pt\"\n",
        "        torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model_base.state_dict(),\n",
        "          'optimizer_state_dict': optimizer_base.state_dict(),\n",
        "          'loss': best_val_loss,\n",
        "          'history': history_base\n",
        "          }, file_path_base)\n",
        "\n",
        "    if patience_counter == patience_limit:\n",
        "      print(f\"Early stopping, model didn't improved for {patience_limit} epochs\")\n",
        "      break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\" + \"-\"*30)\n",
        "  print(\"Training manually interrupted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0223076",
      "metadata": {
        "id": "e0223076"
      },
      "source": [
        "# Train LSTM with attention\n",
        "For LSTM with Attention, i'm considering a smaller **hidden dimension** because the LSTM already has more parameters than the baseline and risks to overfit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d01f8f",
      "metadata": {
        "id": "83d01f8f"
      },
      "outputs": [],
      "source": [
        "lr = config[\"model\"][\"lstm\"][\"lr\"]\n",
        "wd = config[\"model\"][\"lstm\"][\"wd\"]\n",
        "hidden_dim = config[\"model\"][\"lstm\"][\"hidden_dim\"]\n",
        "\n",
        "# 1. Hyperparameters\n",
        "hyps = {\n",
        "    \"embedding_dim\" : embedding_dim, # Embedding dimension\n",
        "    \"hidden_dim\" : hidden_dim, # Hidden dimension\n",
        "    \"output_dim\" : output_dim, # Output dimension\n",
        "    \"lr\" : lr, # Learning rate\n",
        "    \"wd\" : wd-5, #Weight Decay\n",
        "    \"epoch_num\" : epoch_num} # Number of training epoch\n",
        "\n",
        "# 2. Create obj glove weight matrix and model\n",
        "weights = load_glove_embeddings('glove.6B.300d.txt', word2idx, embedding_dim=hyps[\"embedding_dim\"])\n",
        "model_lstm = EmotionClassifierLSTM(len(word2idx), # Vocab Size\n",
        "                                   hyps[\"embedding_dim\"],\n",
        "                                   hyps[\"hidden_dim\"],\n",
        "                                   hyps[\"output_dim\"],\n",
        "                                   weights, # Glove Weights\n",
        "                                   distributions=list(distributions)) # Prior Initialization\n",
        "\n",
        "\n",
        "# 3. Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_lstm = model_lstm.to(device)\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "# Add bias to CrossEntropyLoss\n",
        "freqs = torch.tensor(list(distributions))\n",
        "class_weights = 1.0 / freqs\n",
        "class_weights = class_weights / class_weights.sum() * len(freqs)\n",
        "criterion_lstm = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=hyps['lr'], weight_decay=hyps[\"wd\"])\n",
        "\n",
        "# 5. Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_lstm, mode='min', factor=0.5, patience=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e47e402",
      "metadata": {
        "id": "7e47e402"
      },
      "outputs": [],
      "source": [
        "# Save F1-Score history, both weighted and macro\n",
        "history_lstm = {'weighted': [], 'macro': [], 'train_loss' : [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience_limit = 5\n",
        "\n",
        "# Train loop, calls train and validate custom functions\n",
        "try:\n",
        "  for epoch in range(hyps[\"epoch_num\"]):\n",
        "    # Calls train and validate custom functions\n",
        "    train_loss, train_acc = train(model_lstm, train_loader, device, optimizer_lstm, criterion_lstm)\n",
        "    val_loss, val_acc, f1_weighted, f1_macro, topk_acc = validate(model_lstm, test_loader, device, criterion_lstm)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Epoch [{epoch+1}/{hyps[\"epoch_num\"]}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%,  '\n",
        "          f\"Top-2 Val Accuracy: {topk_acc:.2f}%, \"\n",
        "          f'Weighted F1-Score: {f1_weighted:.4f},  '\n",
        "          f'Macro F1-Score: {f1_macro:.4f},  ')\n",
        "\n",
        "    # Add results to history dictionary\n",
        "    history_lstm['weighted'].append(f1_weighted)\n",
        "    history_lstm['macro'].append(f1_macro)\n",
        "    history_lstm['train_loss'].append(train_loss)\n",
        "    history_lstm['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    patience_counter += 1\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        file_path = f\"./best_model_lstm_attn_v1_f1_{f1_macro:.4f}.pt\"\n",
        "        torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model_lstm.state_dict(),\n",
        "          'optimizer_state_dict': optimizer_lstm.state_dict(),\n",
        "          'loss': best_val_loss,\n",
        "          'history' : history_lstm\n",
        "          }, file_path)\n",
        "\n",
        "    if patience_counter == patience_limit:\n",
        "      print(f\"\\nEarly stopping, model didn't improved for {patience_limit} epochs\")\n",
        "      break\n",
        "\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\" + \"-\"*30)\n",
        "  print(\"Training manually interrupted\")\n",
        "  print(\"-\"*30)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}