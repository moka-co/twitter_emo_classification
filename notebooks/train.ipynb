{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a290f181",
      "metadata": {
        "id": "a290f181"
      },
      "source": [
        "Note: i'm using colab for training, so i need to import my github repository and do all the steps here\n",
        "\n",
        "# Clone Repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/MyLabs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSdA3hA70mlL",
        "outputId": "9ef87ea9-b20d-4483-cb0f-8b025501f0a4"
      },
      "id": "OSdA3hA70mlL",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '$HOME'\n",
            "/content/drive/MyDrive/MyLabs/twitter_emo_classification\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/MyLabs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"twitter_emo_classification\"):\n",
        "  !git clone https://github.com/moka-co/twitter_emo_classification.git\n",
        "\n",
        "%cd twitter_emo_classification\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf41dId7s-ga",
        "outputId": "9c6281b1-24ad-4b20-b060-b9912e3ffcbf"
      },
      "id": "Rf41dId7s-ga",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'twitter_emo_classification'...\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "remote: Counting objects: 100% (186/186), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 186 (delta 92), reused 97 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (186/186), 23.43 MiB | 15.28 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "/content/drive/MyDrive/MyLabs/twitter_emo_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .\n",
        "\n",
        "!chmod +x scripts/run_scripts.sh\n",
        "!./scripts/run_scripts.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyECOjL7tNaU",
        "outputId": "35a7b845-c678-4828-d0d8-e6b20080cc21"
      },
      "id": "iyECOjL7tNaU",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/MyDrive/MyLabs/twitter_emo_classification\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (2.9.0+cpu)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (18.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (3.9.1)\n",
            "Requirement already satisfied: demoji in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (1.1.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (2.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (4.67.1)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (0.3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (3.20.3)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (6.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->emotion-classification==1.0) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->emotion-classification==1.0) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->emotion-classification==1.0) (2025.11.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->emotion-classification==1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->emotion-classification==1.0) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->emotion-classification==1.0) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->emotion-classification==1.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (3.1.6)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->emotion-classification==1.0) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->emotion-classification==1.0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->emotion-classification==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->emotion-classification==1.0) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (1.22.0)\n",
            "Building wheels for collected packages: emotion-classification\n",
            "  Building editable for emotion-classification (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emotion-classification: filename=emotion_classification-1.0-0.editable-py3-none-any.whl size=13558 sha256=98244a71651bd0f84cf4037589a91b0de291a532cdc9a4a127a1d830e585bb54\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-izlc2gnv/wheels/2a/70/a0/a261162a5097e87ca2957fee86eae90e0b89c616c635194788\n",
            "Successfully built emotion-classification\n",
            "Installing collected packages: emotion-classification\n",
            "  Attempting uninstall: emotion-classification\n",
            "    Found existing installation: emotion-classification 1.0\n",
            "    Uninstalling emotion-classification-1.0:\n",
            "      Successfully uninstalled emotion-classification-1.0\n",
            "Successfully installed emotion-classification-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set seed for reproducibility"
      ],
      "metadata": {
        "id": "26zHlmGTvFPj"
      },
      "id": "26zHlmGTvFPj"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "301e763b",
      "metadata": {
        "id": "301e763b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "a72HCTHTvG4x"
      },
      "id": "a72HCTHTvG4x",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a7a193c7",
      "metadata": {
        "id": "a7a193c7"
      },
      "source": [
        "## Vocabulary Construction\n",
        "\n",
        "- Words are initialized with GloVe weight if the word is present\n",
        "- otherwise they are initialized with zero.\n",
        "\n",
        "Considering just a small subset i.e only the words present in the dataset, instead of full GloVe greatly improved the memory footpring from 480 MB (full GloVe) to less for 80k tokens.\n",
        "\n",
        "For Out of Vocabulary words, they are initialized with zero and their embedding are learned during the training.\n",
        "\n",
        "Since the dataset comes from twitter, i expect to have some out of vocabulary words because of twitter slangs.\n",
        "\n",
        "Example: \"sick\" in GloVe (illness) vs tweets (slang for \"awesome\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that defines a glove embeddings matrix\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    path: path to glove.6B.100d.txt\n",
        "    word2idx: dictionary mapping words to integers from your dataset\n",
        "    \"\"\"\n",
        "    vocab_size = len(word2idx)\n",
        "    # Initialize matrix with random values (or zeros)\n",
        "    embedding_matrix = torch.randn(vocab_size, embedding_dim)\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if word in word2idx:\n",
        "                vector = torch.tensor([float(x) for x in values[1:]])\n",
        "                idx = word2idx[word]\n",
        "                embedding_matrix[idx] = vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "4BXCy5S_vPdV"
      },
      "id": "4BXCy5S_vPdV",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7997ede",
      "metadata": {
        "id": "b7997ede",
        "outputId": "db78e493-e44d-440d-bad1-9b61dd610ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocabulary with 79287 tokens.\n"
          ]
        }
      ],
      "source": [
        "# Load vocabulary\n",
        "vocabulary_path = \"/data/vocab.json\"\n",
        "\n",
        "with open(vocabulary_path, 'r', encoding='utf-8') as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "print(f\"Loaded vocabulary with {len(word2idx)} tokens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506ddacd",
      "metadata": {
        "id": "506ddacd"
      },
      "outputs": [],
      "source": [
        "class EmoDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = [torch.tensor(s) for s in sequences]\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a9237e",
      "metadata": {
        "id": "62a9237e",
        "outputId": "75cb08a6-42d5-4783-bf11-2b4f379e4aea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>emotions</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>token_count</th>\n",
              "      <th>sequences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i feel awful about it too because it s my job ...</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "      <td>[i, feel, awful, about, it, too, because, it, ...</td>\n",
              "      <td>26</td>\n",
              "      <td>[2, 3, 455, 28, 13, 94, 38, 13, 85, 11, 330, 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im alone i feel awful</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "      <td>[im, alone, i, feel, awful]</td>\n",
              "      <td>5</td>\n",
              "      <td>[17, 218, 2, 3, 455]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ive probably mentioned this before but i reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>joy</td>\n",
              "      <td>[ive, probably, mentioned, this, before, but, ...</td>\n",
              "      <td>27</td>\n",
              "      <td>[74, 317, 1377, 23, 168, 21, 2, 40, 39, 3, 387...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i was feeling a little low few days back</td>\n",
              "      <td>0</td>\n",
              "      <td>sadness</td>\n",
              "      <td>[i, was, feeling, a, little, low, few, days, b...</td>\n",
              "      <td>9</td>\n",
              "      <td>[2, 22, 8, 7, 56, 405, 190, 165, 101]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i beleive that i am much more sensitive to oth...</td>\n",
              "      <td>2</td>\n",
              "      <td>love</td>\n",
              "      <td>[i, beleive, that, i, am, much, more, sensitiv...</td>\n",
              "      <td>18</td>\n",
              "      <td>[2, 16852, 10, 2, 24, 77, 37, 1808, 5, 120, 15...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label emotions  \\\n",
              "0  i feel awful about it too because it s my job ...      0  sadness   \n",
              "1                              im alone i feel awful      0  sadness   \n",
              "2  ive probably mentioned this before but i reall...      1      joy   \n",
              "3           i was feeling a little low few days back      0  sadness   \n",
              "4  i beleive that i am much more sensitive to oth...      2     love   \n",
              "\n",
              "                                      processed_text  token_count  \\\n",
              "0  [i, feel, awful, about, it, too, because, it, ...           26   \n",
              "1                        [im, alone, i, feel, awful]            5   \n",
              "2  [ive, probably, mentioned, this, before, but, ...           27   \n",
              "3  [i, was, feeling, a, little, low, few, days, b...            9   \n",
              "4  [i, beleive, that, i, am, much, more, sensitiv...           18   \n",
              "\n",
              "                                           sequences  \n",
              "0  [2, 3, 455, 28, 13, 94, 38, 13, 85, 11, 330, 5...  \n",
              "1                               [17, 218, 2, 3, 455]  \n",
              "2  [74, 317, 1377, 23, 168, 21, 2, 40, 39, 3, 387...  \n",
              "3              [2, 22, 8, 7, 56, 405, 190, 165, 101]  \n",
              "4  [2, 16852, 10, 2, 24, 77, 37, 1808, 5, 120, 15...  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_path = \"../data/datasets/final/dataset.parquet\"\n",
        "df = pd.read_parquet(dataset_path)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972729ef",
      "metadata": {
        "id": "972729ef"
      },
      "outputs": [],
      "source": [
        "# Divide dataset into train and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# x = your sequences (list of lists of integers)\n",
        "# y = your labels (0 to 5)\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
        "    df['sequences'].values,\n",
        "    df['label'].values,\n",
        "    test_size=0.2,          # 20% for testing\n",
        "    random_state=42,        # For reproducibility\n",
        "    stratify=df['label'].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1c8071",
      "metadata": {
        "id": "fe1c8071"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Sort by length (optional but helps LSTM efficiency)\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    # Pad sequences to the length of the longest one in this batch\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return padded_sequences, labels\n",
        "\n",
        "# Define seed for reproducibility\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# Create the final DataLoaders\n",
        "BATCH_SIZE=32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    EmoDataset(train_sequences, train_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    EmoDataset(test_sequences, test_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1c374f",
      "metadata": {
        "id": "6e1c374f"
      },
      "outputs": [],
      "source": [
        "# Define train, validate and compute top-k function\n",
        "def train(model, train_loader, device, optimizer, criterion):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "        # Cast labels to long (int64) which is required by CrossEntropyLoss\n",
        "        inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        if hasattr(model, 'attention'):\n",
        "          outputs, _ = model(inputs) # this is different for baseline and lstm model\n",
        "        else:\n",
        "          outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  train_acc = 100 * correct_train / total_train\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "def top_k_accuracy(output, target, k=2):\n",
        "  \"\"\"\n",
        "  Computes the accuracy over the k top predictions\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "      batch_size = target.size(0)\n",
        "\n",
        "      # Get the indices of the top k predictions\n",
        "      # _, pred shape: [batch_size, k]\n",
        "      _, pred = output.topk(k, 1, True, True)\n",
        "\n",
        "      # Transpose to [k, batch_size] to compare with target\n",
        "      pred = pred.t()\n",
        "\n",
        "      # Compare pred with target (target is broadcasted)\n",
        "      # correct shape: [k, batch_size] (Boolean)\n",
        "      correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "      # Sum the correct predictions\n",
        "      correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "      return correct_k.item()\n",
        "\n",
        "\n",
        "def validate(model, test_loader, device, criterion):\n",
        "  model.eval()\n",
        "\n",
        "  val_loss = 0.0 # Validation Loss\n",
        "  correct_val = 0 # Accuracy counter\n",
        "  correct_topk = 0 # Top-k Accuracy counter\n",
        "  total_val = 0 # Total validation loss\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "          # Cast labels to long for validation as well\n",
        "          inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "\n",
        "          # Get output\n",
        "          if hasattr(model, 'attention'):\n",
        "            outputs, _ = model(inputs) # this is different for baseline and lstm model\n",
        "          else:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "          # Standard Validation Loss\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "          # Standard Accuracy (Top-1)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_val += labels.size(0)\n",
        "          correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "          # Top-2 accuracy\n",
        "          correct_topk += top_k_accuracy(outputs, labels, k=2)\n",
        "\n",
        "          all_preds.extend(predicted.cpu().numpy())\n",
        "          all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  val_loss = val_loss / len(test_loader)\n",
        "  val_acc = 100 * correct_val / total_val\n",
        "  topk_acc = 100 * correct_topk / total_val\n",
        "  f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "  f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "  return val_loss, val_acc, f1_weighted, f1_macro, topk_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e83682",
      "metadata": {
        "id": "d1e83682"
      },
      "source": [
        "## Weighted CrossEntropy Loss\n",
        "Cross Entropy Loss is a loss function used for classification, defined as:\n",
        "$$\n",
        "\\ell(\\hat{\\mathbf y}, t) = -\\log \\hat{y}_t = -\\sum_i p(i)\\log \\hat{y}_i\n",
        "$$\n",
        "- $p(i)=1[i-t]$ is one hot vector representing true labels distributions\n",
        "\n",
        "Since the problem is unbalanced, i weighted the cross entropy loss in such way:\n",
        "1. $w_i$ weights are chosen inversely proportional to class frequency\n",
        "2. Cross Entropy Loss is initialized with this bias: `nn.CrossEntropyLoss(weight=class_weights.to(device))`\n",
        "\n",
        "In this way the loss function becomes:\n",
        "$$\n",
        "\\ell(\\hat{\\mathbf y}, t) = - w_t \\log \\hat{y}_t = -\\sum_i w_i \\; p(i)\\log \\hat{y}_i\n",
        "$$\n",
        "\n",
        "This penalizes the model more if it missclassifies rarer classes rather than common ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab32bf6",
      "metadata": {
        "id": "3ab32bf6"
      },
      "outputs": [],
      "source": [
        "# Load TOML configuration for Hyperparameters\n",
        "import tomllib\n",
        "load_path_toml = \"../config.toml\"\n",
        "def load_config(path=load_path_toml):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return tomllib.load(f)\n",
        "\n",
        "config = load_config()\n",
        "\n",
        "# Save shared configuration parameters\n",
        "embedding_dim = config[\"project\"][\"embedding_dim\"]\n",
        "output_dim = config[\"project\"][\"output_dim\"]\n",
        "epoch_num = config[\"project\"][\"epoch_num\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1afb7b5",
      "metadata": {
        "id": "b1afb7b5"
      },
      "source": [
        "# Train Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e6dd2b",
      "metadata": {
        "id": "a0e6dd2b"
      },
      "outputs": [],
      "source": [
        "lr = config[\"model\"][\"baseline\"][\"lr\"]\n",
        "wd = config[\"model\"][\"baseline\"][\"wd\"]\n",
        "hidden_dim = config[\"model\"][\"baseline\"][\"hidden_dim\"]\n",
        "\n",
        "# 1. Hyperparameters\n",
        "hyps = {\n",
        "    \"embedding_dim\" : embedding_dim, # Embedding dimension\n",
        "    \"hidden_dim\" : 256, # Hidden dimension\n",
        "    \"output_dim\" : output_dim, # Output dimension\n",
        "    \"lr\" : lr, # Learning rate\n",
        "    \"wd\" : wd, # weight decay\n",
        "    \"epoch_num\" : epoch_num,\n",
        "    } # Number of training epoch\n",
        "\n",
        "# 2. Create obj glove weight matrix and model\n",
        "weights = load_glove_embeddings('glove.6B.300d.txt', word2idx, embedding_dim=hyps[\"embedding_dim\"])\n",
        "model_base = EmotionClassifierBaseline(len(word2idx), # Dict\n",
        "                                       hyps[\"embedding_dim\"],\n",
        "                                       hyps[\"hidden_dim\"],\n",
        "                                       hyps[\"output_dim\"],\n",
        "                                       weights, # Glove Weights\n",
        "                                       distributions=list(distributions) # Prior Initialization\n",
        "                                       )\n",
        "\n",
        "# 3. Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_base = model_base.to(device)\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "# add bias to Cross Entropy Loss\n",
        "freqs = torch.tensor(list(distributions))\n",
        "class_weights = 1.0 / freqs\n",
        "class_weights = class_weights / class_weights.sum() * len(freqs)\n",
        "criterion_base = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "optimizer_base = torch.optim.Adam(model_base.parameters(), lr=hyps['lr'], weight_decay=hyps[\"wd\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8c6d53",
      "metadata": {
        "id": "5b8c6d53"
      },
      "outputs": [],
      "source": [
        "# Save F1-Score history, both weighted and macro\n",
        "history_base = {'weighted': [], 'macro': [], 'train_loss': [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience_limit = 5\n",
        "\n",
        "# Train loop\n",
        "try:\n",
        "  for epoch in range(hyps[\"epoch_num\"]):\n",
        "    # Calls train and validate custom functions\n",
        "    train_loss, train_acc = train(model_base, train_loader, device, optimizer_base, criterion_base)\n",
        "    val_loss, val_acc, f1_weighted, f1_macro, topk_acc = validate(model_base, test_loader, device, criterion_base)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Epoch [{epoch+1}/{hyps[\"epoch_num\"]}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%,  '\n",
        "          f\"Top-2 Val Accuracy: {topk_acc:.2f}%, \"\n",
        "          f'Weighted F1-Score: {f1_weighted:.4f},  '\n",
        "          f'Macro F1-Score: {f1_macro:.4f},  ')\n",
        "\n",
        "    # Add results to history dictionary\n",
        "    history_base['weighted'].append(f1_weighted)\n",
        "    history_base['macro'].append(f1_macro)\n",
        "    history_base['train_loss'].append(train_loss)\n",
        "    history_base['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    patience_counter += 1\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        file_path_base = f\"./best_model_baseline_v1_f1_{f1_macro:.4f}.pt\"\n",
        "        torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model_base.state_dict(),\n",
        "          'optimizer_state_dict': optimizer_base.state_dict(),\n",
        "          'loss': best_val_loss,\n",
        "          'history': history_base\n",
        "          }, file_path_base)\n",
        "\n",
        "    if patience_counter == patience_limit:\n",
        "      print(f\"Early stopping, model didn't improved for {patience_limit} epochs\")\n",
        "      break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\" + \"-\"*30)\n",
        "  print(\"Training manually interrupted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0223076",
      "metadata": {
        "id": "e0223076"
      },
      "source": [
        "# Train LSTM with attention\n",
        "For LSTM with Attention, i'm considering a smaller **hidden dimension** because the LSTM already has more parameters than the baseline and risks to overfit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d01f8f",
      "metadata": {
        "id": "83d01f8f"
      },
      "outputs": [],
      "source": [
        "lr = config[\"model\"][\"lstm\"][\"lr\"]\n",
        "wd = config[\"model\"][\"lstm\"][\"wd\"]\n",
        "hidden_dim = config[\"model\"][\"lstm\"][\"hidden_dim\"]\n",
        "\n",
        "# 1. Hyperparameters\n",
        "hyps = {\n",
        "    \"embedding_dim\" : embedding_dim, # Embedding dimension\n",
        "    \"hidden_dim\" : hidden_dim, # Hidden dimension\n",
        "    \"output_dim\" : output_dim, # Output dimension\n",
        "    \"lr\" : lr, # Learning rate\n",
        "    \"wd\" : wd-5, #Weight Decay\n",
        "    \"epoch_num\" : epoch_num} # Number of training epoch\n",
        "\n",
        "# 2. Create obj glove weight matrix and model\n",
        "weights = load_glove_embeddings('glove.6B.300d.txt', word2idx, embedding_dim=hyps[\"embedding_dim\"])\n",
        "model_lstm = EmotionClassifierLSTM(len(word2idx), # Vocab Size\n",
        "                                   hyps[\"embedding_dim\"],\n",
        "                                   hyps[\"hidden_dim\"],\n",
        "                                   hyps[\"output_dim\"],\n",
        "                                   weights, # Glove Weights\n",
        "                                   distributions=list(distributions)) # Prior Initialization\n",
        "\n",
        "\n",
        "# 3. Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_lstm = model_lstm.to(device)\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "# Add bias to CrossEntropyLoss\n",
        "freqs = torch.tensor(list(distributions))\n",
        "class_weights = 1.0 / freqs\n",
        "class_weights = class_weights / class_weights.sum() * len(freqs)\n",
        "criterion_lstm = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=hyps['lr'], weight_decay=hyps[\"wd\"])\n",
        "\n",
        "# 5. Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_lstm, mode='min', factor=0.5, patience=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e47e402",
      "metadata": {
        "id": "7e47e402"
      },
      "outputs": [],
      "source": [
        "# Save F1-Score history, both weighted and macro\n",
        "history_lstm = {'weighted': [], 'macro': [], 'train_loss' : [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience_limit = 5\n",
        "\n",
        "# Train loop, calls train and validate custom functions\n",
        "try:\n",
        "  for epoch in range(hyps[\"epoch_num\"]):\n",
        "    # Calls train and validate custom functions\n",
        "    train_loss, train_acc = train(model_lstm, train_loader, device, optimizer_lstm, criterion_lstm)\n",
        "    val_loss, val_acc, f1_weighted, f1_macro, topk_acc = validate(model_lstm, test_loader, device, criterion_lstm)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Epoch [{epoch+1}/{hyps[\"epoch_num\"]}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%,  '\n",
        "          f\"Top-2 Val Accuracy: {topk_acc:.2f}%, \"\n",
        "          f'Weighted F1-Score: {f1_weighted:.4f},  '\n",
        "          f'Macro F1-Score: {f1_macro:.4f},  ')\n",
        "\n",
        "    # Add results to history dictionary\n",
        "    history_lstm['weighted'].append(f1_weighted)\n",
        "    history_lstm['macro'].append(f1_macro)\n",
        "    history_lstm['train_loss'].append(train_loss)\n",
        "    history_lstm['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    patience_counter += 1\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        file_path = f\"./best_model_lstm_attn_v1_f1_{f1_macro:.4f}.pt\"\n",
        "        torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model_lstm.state_dict(),\n",
        "          'optimizer_state_dict': optimizer_lstm.state_dict(),\n",
        "          'loss': best_val_loss,\n",
        "          'history' : history_lstm\n",
        "          }, file_path)\n",
        "\n",
        "    if patience_counter == patience_limit:\n",
        "      print(f\"\\nEarly stopping, model didn't improved for {patience_limit} epochs\")\n",
        "      break\n",
        "\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\" + \"-\"*30)\n",
        "  print(\"Training manually interrupted\")\n",
        "  print(\"-\"*30)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}