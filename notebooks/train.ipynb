{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a290f181",
      "metadata": {
        "id": "a290f181"
      },
      "source": [
        "Note: i'm using colab for training, so i need to import my github repository and do all the steps here\n",
        "\n",
        "# Clone Repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/MyLabs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSdA3hA70mlL",
        "outputId": "3c8f78ee-d7c5-46b2-ab3d-4ffdb155dcc4"
      },
      "id": "OSdA3hA70mlL",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/MyLabs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(\"twitter_emo_classification\"):\n",
        "  !git clone https://github.com/moka-co/twitter_emo_classification.git\n",
        "\n",
        "%cd twitter_emo_classification\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf41dId7s-ga",
        "outputId": "01b75dfb-b45a-45c8-b4c3-35d878fe8589"
      },
      "id": "Rf41dId7s-ga",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MyLabs/twitter_emo_classification\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 2.63 KiB | 2.00 KiB/s, done.\n",
            "From https://github.com/moka-co/twitter_emo_classification\n",
            "   3e699c5..97eb52a  main       -> origin/main\n",
            "Updating 3e699c5..97eb52a\n",
            "Fast-forward\n",
            " notebooks/train.ipynb | 203 \u001b[32m+++++++++++++++++++++++++\u001b[m\u001b[31m-------------------------\u001b[m\n",
            " 1 file changed, 103 insertions(+), 100 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .\n",
        "\n",
        "!chmod +x scripts/run_scripts.sh\n",
        "!./scripts/run_scripts.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyECOjL7tNaU",
        "outputId": "9a15e641-48ec-4e18-a052-27accac7ab1d"
      },
      "id": "iyECOjL7tNaU",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/MyDrive/MyLabs/twitter_emo_classification\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (18.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (3.9.1)\n",
            "Collecting demoji (from emotion-classification==1.0)\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting emoji (from emotion-classification==1.0)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (4.67.1)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from emotion-classification==1.0) (0.3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (3.20.3)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->emotion-classification==1.0) (6.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->emotion-classification==1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->emotion-classification==1.0) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->emotion-classification==1.0) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->emotion-classification==1.0) (2025.11.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->emotion-classification==1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->emotion-classification==1.0) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->emotion-classification==1.0) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->emotion-classification==1.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->emotion-classification==1.0) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->emotion-classification==1.0) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->emotion-classification==1.0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->emotion-classification==1.0) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->emotion-classification==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->emotion-classification==1.0) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->emotion-classification==1.0) (1.22.0)\n",
            "Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: emotion-classification\n",
            "  Building editable for emotion-classification (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emotion-classification: filename=emotion_classification-1.0-0.editable-py3-none-any.whl size=13558 sha256=1b86d7242af413cd2d14727b78eb759820a9d09648f8a91abd2ace71375b0a9a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kygiuzb1/wheels/2a/70/a0/a261162a5097e87ca2957fee86eae90e0b89c616c635194788\n",
            "Successfully built emotion-classification\n",
            "Installing collected packages: emoji, demoji, emotion-classification\n",
            "Successfully installed demoji-1.1.0 emoji-2.15.0 emotion-classification-1.0\n",
            "Successfully download and extracted Glove model under data/glove/ directory\n",
            "Semeval dataset already present under data/datasets/raw\n",
            "Successfully downloaded ELTEA17 Dataset under data/datasets/raw\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/aadyasingh55/twitter-emotion-classification-dataset?dataset_version_number=1...\n",
            "100% 22.3M/22.3M [00:02<00:00, 8.29MB/s]\n",
            "Extracting files...\n",
            "Filtered 591 rows out of 6838\n",
            "                                                text  label emotions\n",
            "0  i feel awful about it too because it s my job ...      0  sadness\n",
            "1                              im alone i feel awful      0  sadness\n",
            "2  ive probably mentioned this before but i reall...      1      joy\n",
            "3           i was feeling a little low few days back      0  sadness\n",
            "4  i beleive that i am much more sensitive to oth...      2     love\n",
            "Database successfully saved under /content/drive/MyDrive/MyLabs/twitter_emo_classification/data/datasets/process/merged_emotions.parquet\n",
            "Applying Preprocessing Pipeline...\n",
            "Success! Processed 429303 rows in 96.60s\n",
            "\n",
            "--- Statistics ---\n",
            "Token Mean: 19.12\n",
            "Token Std:  10.96\n",
            "Outlier Delimiter (3-sigma): 52 tokens\n",
            "\n",
            "--- Filtering ---\n",
            "Original length: 429303\n",
            "Filtered length: 425258\n",
            "Deleted samples: 4045\n",
            "Database successfully saved\n",
            "Vocab built. Size: 79287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "301e763b",
      "metadata": {
        "id": "301e763b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import f1_score, confusion_matrix, matthews_corrcoef, classification_report\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set seed for reproducibility"
      ],
      "metadata": {
        "id": "26zHlmGTvFPj"
      },
      "id": "26zHlmGTvFPj"
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "a72HCTHTvG4x"
      },
      "id": "a72HCTHTvG4x",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a7a193c7",
      "metadata": {
        "id": "a7a193c7"
      },
      "source": [
        "## Vocabulary Construction\n",
        "\n",
        "- Words are initialized with GloVe weight if the word is present\n",
        "- otherwise they are initialized with zero.\n",
        "\n",
        "Considering just a small subset i.e only the words present in the dataset, instead of full GloVe greatly improved the memory footpring from 480 MB (full GloVe) to less for 80k tokens.\n",
        "\n",
        "For Out of Vocabulary words, they are initialized with zero and their embedding are learned during the training.\n",
        "\n",
        "Since the dataset comes from twitter, i expect to have some out of vocabulary words because of twitter slangs.\n",
        "\n",
        "Example: \"sick\" in GloVe (illness) vs tweets (slang for \"awesome\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that defines a glove embeddings matrix\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    path: path to glove.6B.100d.txt\n",
        "    word2idx: dictionary mapping words to integers from your dataset\n",
        "    \"\"\"\n",
        "    vocab_size = len(word2idx)\n",
        "    # Initialize matrix with random values (or zeros)\n",
        "    embedding_matrix = torch.randn(vocab_size, embedding_dim)\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if word in word2idx:\n",
        "                vector = torch.tensor([float(x) for x in values[1:]])\n",
        "                idx = word2idx[word]\n",
        "                embedding_matrix[idx] = vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "# Define Class Wrapper for Dataset\n",
        "class EmoDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = [torch.tensor(s) for s in sequences]\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "4BXCy5S_vPdV"
      },
      "id": "4BXCy5S_vPdV",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b7997ede",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7997ede",
        "outputId": "0e99ba08-a87b-433b-dfe2-3be5f7c9c4d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vocabulary with 79287 tokens.\n"
          ]
        }
      ],
      "source": [
        "# Load vocabulary\n",
        "vocabulary_path = \"data/vocab.json\"\n",
        "\n",
        "with open(vocabulary_path, 'r', encoding='utf-8') as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "print(f\"Loaded vocabulary with {len(word2idx)} tokens.\")\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"data/datasets/final/dataset.parquet\"\n",
        "df = pd.read_parquet(dataset_path)\n",
        "\n",
        "\n",
        "# Divide dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "# x = your sequences (list of lists of integers)\n",
        "# y = your labels (0 to 5)\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
        "    df['sequences'].values,\n",
        "    df['label'].values,\n",
        "    test_size=0.2,          # 20% for testing\n",
        "    random_state=42,        # For reproducibility\n",
        "    stratify=df['label'].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fe1c8071",
      "metadata": {
        "id": "fe1c8071"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Sort by length (optional but helps LSTM efficiency)\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    # Pad sequences to the length of the longest one in this batch\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return padded_sequences, labels\n",
        "\n",
        "# Define seed for reproducibility\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# Create the final DataLoaders\n",
        "BATCH_SIZE=32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    EmoDataset(train_sequences, train_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    EmoDataset(test_sequences, test_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6e1c374f",
      "metadata": {
        "id": "6e1c374f"
      },
      "outputs": [],
      "source": [
        "# Define train, validate and compute top-k function\n",
        "def train(model, train_loader, device, optimizer, criterion):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "        # Cast labels to long (int64) which is required by CrossEntropyLoss\n",
        "        inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        if hasattr(model, 'attention'):\n",
        "          outputs, _ = model(inputs) # this is different for baseline and lstm model\n",
        "        else:\n",
        "          outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  train_acc = 100 * correct_train / total_train\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "def top_k_accuracy(output, target, k=2):\n",
        "  \"\"\"\n",
        "  Computes the accuracy over the k top predictions\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "      batch_size = target.size(0)\n",
        "\n",
        "      # Get the indices of the top k predictions\n",
        "      # _, pred shape: [batch_size, k]\n",
        "      _, pred = output.topk(k, 1, True, True)\n",
        "\n",
        "      # Transpose to [k, batch_size] to compare with target\n",
        "      pred = pred.t()\n",
        "\n",
        "      # Compare pred with target (target is broadcasted)\n",
        "      # correct shape: [k, batch_size] (Boolean)\n",
        "      correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "      # Sum the correct predictions\n",
        "      correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "      return correct_k.item()\n",
        "\n",
        "\n",
        "def validate(model, test_loader, device, criterion):\n",
        "  model.eval()\n",
        "\n",
        "  val_loss = 0.0 # Validation Loss\n",
        "  correct_val = 0 # Accuracy counter\n",
        "  correct_topk = 0 # Top-k Accuracy counter\n",
        "  total_val = 0 # Total validation loss\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "          # Cast labels to long for validation as well\n",
        "          inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "\n",
        "          # Get output\n",
        "          if hasattr(model, 'attention'):\n",
        "            outputs, _ = model(inputs) # this is different for baseline and lstm model\n",
        "          else:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "          # Standard Validation Loss\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "          # Standard Accuracy (Top-1)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_val += labels.size(0)\n",
        "          correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "          # Top-2 accuracy\n",
        "          correct_topk += top_k_accuracy(outputs, labels, k=2)\n",
        "\n",
        "          all_preds.extend(predicted.cpu().numpy())\n",
        "          all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  val_loss = val_loss / len(test_loader)\n",
        "  val_acc = 100 * correct_val / total_val\n",
        "  topk_acc = 100 * correct_topk / total_val\n",
        "  f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "  f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "  return val_loss, val_acc, f1_weighted, f1_macro, topk_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e83682",
      "metadata": {
        "id": "d1e83682"
      },
      "source": [
        "## Weighted CrossEntropy Loss\n",
        "Cross Entropy Loss is a loss function used for classification, defined as:\n",
        "$$\n",
        "\\ell(\\hat{\\mathbf y}, t) = -\\log \\hat{y}_t = -\\sum_i p(i)\\log \\hat{y}_i\n",
        "$$\n",
        "- $p(i)=1[i-t]$ is one hot vector representing true labels distributions\n",
        "\n",
        "Since the problem is unbalanced, i weighted the cross entropy loss in such way:\n",
        "1. $w_i$ weights are chosen inversely proportional to class frequency\n",
        "2. Cross Entropy Loss is initialized with this bias: `nn.CrossEntropyLoss(weight=class_weights.to(device))`\n",
        "\n",
        "In this way the loss function becomes:\n",
        "$$\n",
        "\\ell(\\hat{\\mathbf y}, t) = - w_t \\log \\hat{y}_t = -\\sum_i w_i \\; p(i)\\log \\hat{y}_i\n",
        "$$\n",
        "\n",
        "This penalizes the model more if it missclassifies rarer classes rather than common ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3ab32bf6",
      "metadata": {
        "id": "3ab32bf6"
      },
      "outputs": [],
      "source": [
        "# Load TOML configuration for Hyperparameters\n",
        "import tomllib\n",
        "load_path_toml = \"config.toml\"\n",
        "def load_config(path=load_path_toml):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return tomllib.load(f)\n",
        "\n",
        "config = load_config()\n",
        "\n",
        "# Save shared configuration parameters\n",
        "embedding_dim = config[\"project\"][\"embedding_dim\"]\n",
        "output_dim = config[\"project\"][\"output_dim\"]\n",
        "epoch_num = config[\"project\"][\"epoch_num\"]\n",
        "\n",
        "\n",
        "# Compute distributions\n",
        "distributions=df['emotions'].value_counts(normalize=True)\n",
        "\n",
        "# Create directory where to save models\n",
        "if not os.path.exists(\"models\"):\n",
        "  os.makedirs(\"models\")\n",
        "\n",
        "# Set glove path\n",
        "glove_path = 'data/glove/glove.6B.300d.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1afb7b5",
      "metadata": {
        "id": "b1afb7b5"
      },
      "source": [
        "# Train Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a0e6dd2b",
      "metadata": {
        "id": "a0e6dd2b"
      },
      "outputs": [],
      "source": [
        "from src.models.EmoBaseline import EmoBaseline\n",
        "\n",
        "lr = config[\"model\"][\"baseline\"][\"lr\"]\n",
        "wd = config[\"model\"][\"baseline\"][\"wd\"]\n",
        "hidden_dim = config[\"model\"][\"baseline\"][\"hidden_dim\"]\n",
        "\n",
        "# 1. Hyperparameters\n",
        "hyps = {\n",
        "    \"embedding_dim\" : embedding_dim, # Embedding dimension\n",
        "    \"hidden_dim\" : hidden_dim, # Hidden dimension\n",
        "    \"output_dim\" : output_dim, # Output dimension\n",
        "    \"lr\" : lr, # Learning rate\n",
        "    \"wd\" : wd, # weight decay\n",
        "    \"epoch_num\" : epoch_num,\n",
        "    } # Number of training epoch\n",
        "\n",
        "# 2. Create obj glove weight matrix and model\n",
        "weights = load_glove_embeddings(glove_path, word2idx, embedding_dim=hyps[\"embedding_dim\"])\n",
        "model_base = EmoBaseline(len(word2idx), # Dict\n",
        "                                       hyps[\"embedding_dim\"],\n",
        "                                       hyps[\"hidden_dim\"],\n",
        "                                       hyps[\"output_dim\"],\n",
        "                                       weights, # Glove Weights\n",
        "                                       distributions=list(distributions) # Prior Initialization\n",
        "                                       )\n",
        "\n",
        "# 3. Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_base = model_base.to(device)\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "# add bias to Cross Entropy Loss\n",
        "freqs = torch.tensor(list(distributions))\n",
        "class_weights = 1.0 / freqs\n",
        "class_weights = class_weights / class_weights.sum() * len(freqs)\n",
        "criterion_base = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "optimizer_base = torch.optim.Adam(model_base.parameters(), lr=hyps['lr'], weight_decay=hyps[\"wd\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "5b8c6d53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8c6d53",
        "outputId": "cbc3f1b7-f667-4add-c46a-7fe1d314203d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Train Loss: 1.1096, Train Acc: 55.93%, Val Loss: 0.4311, Val Acc: 86.41%,  Top-2 Val Accuracy: 95.96%, Weighted F1-Score: 0.8654,  Macro F1-Score: 0.8247,  \n",
            "Epoch [2/20], Train Loss: 0.3820, Train Acc: 87.50%, Val Loss: 0.3347, Val Acc: 88.30%,  Top-2 Val Accuracy: 96.62%, Weighted F1-Score: 0.8862,  Macro F1-Score: 0.8523,  \n",
            "Epoch [3/20], Train Loss: 0.3175, Train Acc: 88.68%, Val Loss: 0.3131, Val Acc: 88.56%,  Top-2 Val Accuracy: 97.23%, Weighted F1-Score: 0.8883,  Macro F1-Score: 0.8573,  \n",
            "Epoch [4/20], Train Loss: 0.2823, Train Acc: 89.35%, Val Loss: 0.3140, Val Acc: 88.77%,  Top-2 Val Accuracy: 97.93%, Weighted F1-Score: 0.8891,  Macro F1-Score: 0.8471,  \n",
            "Epoch [5/20], Train Loss: 0.2629, Train Acc: 89.64%, Val Loss: 0.2686, Val Acc: 89.23%,  Top-2 Val Accuracy: 98.35%, Weighted F1-Score: 0.8942,  Macro F1-Score: 0.8586,  \n",
            "Epoch [6/20], Train Loss: 0.2505, Train Acc: 89.77%, Val Loss: 0.3051, Val Acc: 88.30%,  Top-2 Val Accuracy: 97.68%, Weighted F1-Score: 0.8893,  Macro F1-Score: 0.8401,  \n",
            "Epoch [7/20], Train Loss: 0.2393, Train Acc: 89.95%, Val Loss: 0.2533, Val Acc: 89.36%,  Top-2 Val Accuracy: 98.51%, Weighted F1-Score: 0.8965,  Macro F1-Score: 0.8627,  \n",
            "Epoch [8/20], Train Loss: 0.2324, Train Acc: 89.95%, Val Loss: 0.2677, Val Acc: 88.85%,  Top-2 Val Accuracy: 98.06%, Weighted F1-Score: 0.8903,  Macro F1-Score: 0.8530,  \n",
            "\n",
            "------------------------------\n",
            "Training manually interrupted\n"
          ]
        }
      ],
      "source": [
        "# Save F1-Score history, both weighted and macro\n",
        "history_base = {'weighted': [], 'macro': [], 'train_loss': [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience_limit = 5\n",
        "\n",
        "# Train loop\n",
        "try:\n",
        "  for epoch in range(hyps[\"epoch_num\"]):\n",
        "    # Calls train and validate custom functions\n",
        "    train_loss, train_acc = train(model_base, train_loader, device, optimizer_base, criterion_base)\n",
        "    val_loss, val_acc, f1_weighted, f1_macro, topk_acc = validate(model_base, test_loader, device, criterion_base)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Epoch [{epoch+1}/{hyps[\"epoch_num\"]}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%,  '\n",
        "          f\"Top-2 Val Accuracy: {topk_acc:.2f}%, \"\n",
        "          f'Weighted F1-Score: {f1_weighted:.4f},  '\n",
        "          f'Macro F1-Score: {f1_macro:.4f},  ')\n",
        "\n",
        "    # Add results to history dictionary\n",
        "    history_base['weighted'].append(f1_weighted)\n",
        "    history_base['macro'].append(f1_macro)\n",
        "    history_base['train_loss'].append(train_loss)\n",
        "    history_base['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    patience_counter += 1\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        file_path_base = f\"./models/best_model_baseline_v1_f1_{f1_macro:.4f}.pt\"\n",
        "        torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model_base.state_dict(),\n",
        "          'optimizer_state_dict': optimizer_base.state_dict(),\n",
        "          'loss': best_val_loss,\n",
        "          'history': history_base\n",
        "          }, file_path_base)\n",
        "\n",
        "    if patience_counter == patience_limit:\n",
        "      print(f\"Early stopping, model didn't improved for {patience_limit} epochs\")\n",
        "      break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\" + \"-\"*30)\n",
        "  print(\"Training manually interrupted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0223076",
      "metadata": {
        "id": "e0223076"
      },
      "source": [
        "# Train LSTM with attention\n",
        "For LSTM with Attention, i'm considering a smaller **hidden dimension** because the LSTM already has more parameters than the baseline and risks to overfit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "83d01f8f",
      "metadata": {
        "id": "83d01f8f"
      },
      "outputs": [],
      "source": [
        "from src.models.EmoLSTM import EmoLSTM\n",
        "\n",
        "lr = config[\"model\"][\"lstm\"][\"lr\"]\n",
        "wd = config[\"model\"][\"lstm\"][\"wd\"]\n",
        "hidden_dim = config[\"model\"][\"lstm\"][\"hidden_dim\"]\n",
        "\n",
        "# 1. Hyperparameters\n",
        "hyps = {\n",
        "    \"embedding_dim\" : embedding_dim, # Embedding dimension\n",
        "    \"hidden_dim\" : hidden_dim, # Hidden dimension\n",
        "    \"output_dim\" : output_dim, # Output dimension\n",
        "    \"lr\" : lr, # Learning rate\n",
        "    \"wd\" : wd, #Weight Decay\n",
        "    \"epoch_num\" : epoch_num} # Number of training epoch\n",
        "\n",
        "# 2. Create obj glove weight matrix and model\n",
        "weights = load_glove_embeddings(glove_path, word2idx, embedding_dim=hyps[\"embedding_dim\"])\n",
        "model_lstm = EmoLSTM(len(word2idx), # Vocab Size\n",
        "                                   hyps[\"embedding_dim\"],\n",
        "                                   hyps[\"hidden_dim\"],\n",
        "                                   hyps[\"output_dim\"],\n",
        "                                   weights, # Glove Weights\n",
        "                                   distributions=list(distributions)) # Prior Initialization\n",
        "\n",
        "\n",
        "# 3. Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_lstm = model_lstm.to(device)\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "# Add bias to CrossEntropyLoss\n",
        "freqs = torch.tensor(list(distributions))\n",
        "class_weights = 1.0 / freqs\n",
        "class_weights = class_weights / class_weights.sum() * len(freqs)\n",
        "criterion_lstm = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=hyps['lr'], weight_decay=hyps[\"wd\"])\n",
        "\n",
        "# 5. Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_lstm, mode='min', factor=0.5, patience=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7e47e402",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e47e402",
        "outputId": "4301dd1a-d915-46da-8513-5b314fa943d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Train Loss: 0.2132, Train Acc: 90.55%, Val Loss: 0.1377, Val Acc: 93.31%,  Top-2 Val Accuracy: 99.28%, Weighted F1-Score: 0.9349,  Macro F1-Score: 0.9078,  \n",
            "Epoch [2/20], Train Loss: 0.1317, Train Acc: 93.56%, Val Loss: 0.1339, Val Acc: 93.41%,  Top-2 Val Accuracy: 99.48%, Weighted F1-Score: 0.9358,  Macro F1-Score: 0.9083,  \n",
            "Epoch [3/20], Train Loss: 0.1238, Train Acc: 93.75%, Val Loss: 0.1317, Val Acc: 93.56%,  Top-2 Val Accuracy: 99.46%, Weighted F1-Score: 0.9375,  Macro F1-Score: 0.9101,  \n",
            "\n",
            "------------------------------\n",
            "Training manually interrupted\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Save F1-Score history, both weighted and macro\n",
        "history_lstm = {'weighted': [], 'macro': [], 'train_loss' : [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience_limit = 5\n",
        "\n",
        "# Train loop, calls train and validate custom functions\n",
        "try:\n",
        "  for epoch in range(hyps[\"epoch_num\"]):\n",
        "    # Calls train and validate custom functions\n",
        "    train_loss, train_acc = train(model_lstm, train_loader, device, optimizer_lstm, criterion_lstm)\n",
        "    val_loss, val_acc, f1_weighted, f1_macro, topk_acc = validate(model_lstm, test_loader, device, criterion_lstm)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Epoch [{epoch+1}/{hyps[\"epoch_num\"]}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%,  '\n",
        "          f\"Top-2 Val Accuracy: {topk_acc:.2f}%, \"\n",
        "          f'Weighted F1-Score: {f1_weighted:.4f},  '\n",
        "          f'Macro F1-Score: {f1_macro:.4f},  ')\n",
        "\n",
        "    # Add results to history dictionary\n",
        "    history_lstm['weighted'].append(f1_weighted)\n",
        "    history_lstm['macro'].append(f1_macro)\n",
        "    history_lstm['train_loss'].append(train_loss)\n",
        "    history_lstm['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    patience_counter += 1\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        file_path = f\"models/best_model_lstm_attn_v1_f1_{f1_macro:.4f}.pt\"\n",
        "        torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model_lstm.state_dict(),\n",
        "          'optimizer_state_dict': optimizer_lstm.state_dict(),\n",
        "          'loss': best_val_loss,\n",
        "          'history' : history_lstm\n",
        "          }, file_path)\n",
        "\n",
        "    if patience_counter == patience_limit:\n",
        "      print(f\"\\nEarly stopping, model didn't improved for {patience_limit} epochs\")\n",
        "      break\n",
        "\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\" + \"-\"*30)\n",
        "  print(\"Training manually interrupted\")\n",
        "  print(\"-\"*30)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}